druid.service=druid/dev/middleManager
druid.port=8091

# Store task logs in deep storage
#druid.indexer.logs.type=file
#druid.indexer.logs.directory=/log/druid/indexer
druid.indexer.logs.type=hdfs
druid.indexer.logs.directory=hdfs://emn-g04-03:9000/druid/logs


# Number of tasks per middleManager
druid.worker.capacity=3

# Task launch parameters
druid.indexer.runner.javaOpts=-server -Xmx2g -Duser.timezone=UTC -Dfile.encoding=UTF-8 -Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager
druid.indexer.task.baseTaskDir=var/druid/task

# HTTP server threads
druid.server.http.numThreads=12

# Processing threads and buffers
druid.processing.buffer.sizeBytes=256000000
druid.processing.numThreads=2

# Hadoop indexing
druid.indexer.task.hadoopWorkingPath=var/druid/hadoop-tmp
druid.indexer.task.defaultHadoopCoordinates=["org.apache.hadoop:hadoop-client:2.3.0"]

# Peon properties
druid.indexer.fork.property.druid.monitoring.monitors=["com.metamx.metrics.JvmMonitor"]
druid.indexer.fork.property.druid.processing.buffer.sizeBytes=536870912
druid.indexer.fork.property.druid.processing.numThreads=2
druid.indexer.fork.property.druid.segmentCache.locations=[{"path": "/data1/druid/zk_druid", "maxSize": 0}]
druid.indexer.fork.property.druid.server.http.numThreads=50

druid.worker.capacity=20
druid.worker.ip=localhost
druid.worker.version=0
